\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{MathModel}

\begin{document}
\maketitle

\section{Двошарова нейронна мережа з ReLU}

\subsection{Параметри мережі}
Розглянемо невелику нейронну мережу з такими параметрами:
\begin{itemize}
    \item Вхідний вектор: $\mathbf{x} = \begin{pmatrix} 1 \\ 2 \\ 1 \end{pmatrix}$ розмірності $3 \times 1$
    \item Перший шар: 2 нейрони
    \item Другий шар: 1 нейрон
    \item Функція активації: ReLU($x$) = max(0, $x$)
\end{itemize}

\subsection{Ініціалізація вагів (He initialization)}
Для шару з $n_{in}$ входами, ваги ініціалізуються як $\mathcal{N}(0, \sqrt{\frac{2}{n_{in}}})$


\section{Ініціалізація He для першого шару}

\subsection{Параметри розподілу}
\begin{align*}
\mu &= 0 \\
\sigma &= \sqrt{\frac{2}{n_{in}}} = \sqrt{\frac{2}{3}} \approx 0.816
\end{align*}

\subsection{Генерація елементів матриці}
Кожен елемент матриці $W_1$ генерується як випадкове число з розподілу $\mathcal{N}(0, 0.816)$:

\[W_1 = \begin{pmatrix} 
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23}
\end{pmatrix}\]

де кожен елемент $w_{ij}$ генерується так:
\begin{align*}
w_{11} &= 0.4 \cdot 0.816 \approx 0.326 \\
w_{12} &= -0.5 \cdot 0.816 \approx -0.408 \\
w_{13} &= 0.1 \cdot 0.816 \approx 0.082 \\
w_{21} &= 0.3 \cdot 0.816 \approx 0.245 \\
w_{22} &= 0.2 \cdot 0.816 \approx 0.163 \\
w_{23} &= -0.4 \cdot 0.816 \approx -0.326
\end{align*}

\subsection{Пояснення}
1. Спочатку генеруються випадкові числа з стандартного нормального розподілу $\mathcal{N}(0, 1)$:
   \begin{pmatrix} 
   0.4 & -0.5 & 0.1 \\
   0.3 & 0.2 & -0.4
   \end{pmatrix}

2. Потім ці числа множаться на $\sqrt{\frac{2}{3}}$, щоб отримати правильне стандартне відхилення:
   \[\text{final\_value} = \text{random\_normal}(0,1) \cdot \sqrt{\frac{2}{3}}\]

\subsection{Важливість ініціалізації He}
Ініціалізація He допомагає:
\begin{itemize}
    \item Запобігти проблемі зникаючих/вибухаючих градієнтів
    \item Підтримувати дисперсію активацій приблизно однаковою в усіх шарах
    \item Особливо ефективна для ReLU активації
\end{itemize}



\textbf{Перший шар} ($W_1$): $n_{in} = 3$
\[W_1 = \sqrt{\frac{2}{3}} \cdot \begin{pmatrix} 
0.4 & -0.5 & 0.1 \\
0.3 & 0.2 & -0.4
\end{pmatrix}\]
\[b_1 = \begin{pmatrix} 0.1 \\ -0.2 \end{pmatrix}\]

\textbf{Другий шар} ($W_2$): $n_{in} = 2$
\[W_2 = \sqrt{\frac{2}{2}} \cdot \begin{pmatrix} 0.3 & 0.6 \end{pmatrix}\]
\[b_2 = \begin{pmatrix} 0.1 \end{pmatrix}\]
--------------------------------------------------------

\section{Визначення розмірів матриць ваг та біасів}

\subsection{Загальний принцип}
Для кожного шару $l$:
\begin{itemize}
    \item Розмір матриці ваг $W_l$: $(n_l \times n_{l-1})$
    \item де $n_l$ - кількість нейронів в поточному шарі
    \item $n_{l-1}$ - кількість нейронів в попередньому шарі
    \item Розмір вектора біасів $b_l$: $(n_l \times 1)$
\end{itemize}

\subsection{Перший шар ($W_1$)}
\begin{align*}
\text{Вхідний шар (l-1):} & \quad n_0 = 3 \text{ (розмір вхідного вектора)} \\
\text{Перший шар (l):} & \quad n_1 = 2 \text{ (кількість нейронів)} \\
\text{Розмір } W_1: & \quad 2 \times 3 \\
\text{Розмір } b_1: & \quad 2 \times 1
\end{align*}

\[W_1 = \begin{pmatrix} 
w_{11} & w_{12} & w_{13} \\
w_{21} & w_{22} & w_{23}
\end{pmatrix} \quad b_1 = \begin{pmatrix} b_1 \\ b_2 \end{pmatrix}\]

\textbf{Пояснення розміру:}
\begin{itemize}
    \item 2 рядки: кожен нейрон першого шару потребує свій набір ваг
    \item 3 стовпці: кожен нейрон отримує 3 входи
\end{itemize}

\subsection{Другий шар ($W_2$)}
\begin{align*}
\text{Попередній шар (l-1):} & \quad n_1 = 2 \text{ (виходи першого шару)} \\
\text{Другий шар (l):} & \quad n_2 = 1 \text{ (один вихідний нейрон)} \\
\text{Розмір } W_2: & \quad 1 \times 2 \\
\text{Розмір } b_2: & \quad 1 \times 1
\end{align*}

\[W_2 = \begin{pmatrix} w_{11} & w_{12} \end{pmatrix} \quad b_2 = \begin{pmatrix} b_1 \end{pmatrix}\]

\textbf{Пояснення розміру:}
\begin{itemize}
    \item 1 рядок: один вихідний нейрон
    \item 2 стовпці: отримує 2 входи з першого шару
\end{itemize}

\subsection{Ініціалізація біасів}
Біаси зазвичай ініціалізуються:
\begin{itemize}
    \item Нулями: $b_l = \vec{0}$
    \item Малими випадковими числами: $b_l \sim \mathcal{N}(0, 0.01)$
    \item Константами: $b_l = 0.01$
\end{itemize}

У нашому прикладі:
\begin{align*}
b_1 &= \begin{pmatrix} 0.1 \\ -0.2 \end{pmatrix} \\
b_2 &= \begin{pmatrix} 0.1 \end{pmatrix}
\end{align*}

\subsection{Візуалізація розмірностей для forward propagation}
Для першого шару:
\[\underbrace{(2 \times 3)}_{W_1} \cdot \underbrace{(3 \times 1)}_{\mathbf{x}} + \underbrace{(2 \times 1)}_{b_1} = \underbrace{(2 \times 1)}_{z_1}\]

Для другого шару:
\[\underbrace{(1 \times 2)}_{W_2} \cdot \underbrace{(2 \times 1)}_{a_1} + \underbrace{(1 \times 1)}_{b_2} = \underbrace{(1 \times 1)}_{z_2}\]

--------------------------------------------------------
\subsection{Forward Propagation}

\textbf{1. Перший шар:}
\begin{align*}
z_1 &= W_1\mathbf{x} + b_1 \\
&= \begin{pmatrix} 
0.4 & -0.5 & 0.1 \\
0.3 & 0.2 & -0.4
\end{pmatrix} \begin{pmatrix} 1 \\ 2 \\ 1 \end{pmatrix} + 
\begin{pmatrix} 0.1 \\ -0.2 \end{pmatrix} \\
&= \begin{pmatrix} -0.5 \\ 0.4 \end{pmatrix}
\end{align*}

\textbf{Застосування ReLU:}
\[a_1 = \text{ReLU}(z_1) = \begin{pmatrix} 0 \\ 0.4 \end{pmatrix}\]

\textbf{2. Другий шар:}
\begin{align*}
z_2 &= W_2a_1 + b_2 \\
&= \begin{pmatrix} 0.3 & 0.6 \end{pmatrix} \begin{pmatrix} 0 \\ 0.4 \end{pmatrix} + 0.1 \\
&= 0.34
\end{align*}

\textbf{Вихід мережі:}
\[a_2 = \text{ReLU}(z_2) = 0.34\]

\subsection{Backpropagation}

Нехай цільове значення $y = 1$. Використаємо MSE як функцію втрат:
\[L = \frac{1}{2}(y - a_2)^2 = \frac{1}{2}(1 - 0.34)^2 = 0.217\]

\textbf{1. Градієнти другого шару:}
\[\frac{\partial L}{\partial a_2} = -(y - a_2) = -0.66\]
\[\frac{\partial a_2}{\partial z_2} = \begin{cases} 1 & \text{if } z_2 > 0 \\ 0 & \text{otherwise} \end{cases} = 1\]
\[\frac{\partial L}{\partial z_2} = \frac{\partial L}{\partial a_2}\frac{\partial a_2}{\partial z_2} = -0.66\]

\[\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial z_2}\begin{pmatrix} a_1^T \end{pmatrix} = -0.66 \begin{pmatrix} 0 & 0.4 \end{pmatrix}\]
\[\frac{\partial L}{\partial b_2} = \frac{\partial L}{\partial z_2} = -0.66\]

\textbf{2. Градієнти першого шару:}
\[\delta_1 = W_2^T\frac{\partial L}{\partial z_2}\odot\text{ReLU}'(z_1)\]
де $\text{ReLU}'(z_1)$ - похідна ReLU:
\[\text{ReLU}'(z_1) = \begin{pmatrix} 0 \\ 1 \end{pmatrix}\]

\[\delta_1 = \begin{pmatrix} 0.3 \\ 0.6 \end{pmatrix} (-0.66) \odot \begin{pmatrix} 0 \\ 1 \end{pmatrix} = \begin{pmatrix} 0 \\ -0.396 \end{pmatrix}\]

\[\frac{\partial L}{\partial W_1} = \delta_1\mathbf{x}^T\]
\[\frac{\partial L}{\partial b_1} = \delta_1\]

\subsection{Оновлення вагів}
З швидкістю навчання $\alpha = 0.1$:
\begin{align*}
W_2 &= W_2 - \alpha\frac{\partial L}{\partial W_2} \\
b_2 &= b_2 - \alpha\frac{\partial L}{\partial b_2} \\
W_1 &= W_1 - \alpha\frac{\partial L}{\partial W_1} \\
b_1 &= b_1 - \alpha\frac{\partial L}{\partial b_1}
\end{align*}

\end{document}

\end{document}